{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file,index_col='id', parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = import_data('../input/train.csv')#, index_col='id')#, nrows=2505542)\ntest = import_data('../input/test.csv')#, index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = pd.read_csv('../input/train.csv', index_col='id')\n#test = pd.read_csv('../input/test.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train.shape)\nprint (test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file,nrows=100_000, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file,nrows=100_000, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df"},{"metadata":{"trusted":true},"cell_type":"code","source":"structures = import_data('../input/structures.csv')#, nrows=20_000)\ndisplay(structures.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"structures.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Map the atom structure data into train and test files\n\ndef map_atom_info(df, atom_idx):\n    df = pd.merge(df, structures, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    \n    df = df.drop('atom_index', axis=1)\n    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n                            'x': f'x_{atom_idx}',\n                            'y': f'y_{atom_idx}',\n                            'z': f'z_{atom_idx}'})\n    return df\n\ntrain = map_atom_info(train, 0)\ntrain = map_atom_info(train, 1)\n\ntest = map_atom_info(test, 0)\ntest = map_atom_info(test, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Engineer a single feature: distance vector between atoms\n#  (there's ways to speed this up!)\n\ndef dist(row):\n    return ( (row['x_1'] - row['x_0'])**2 +\n             (row['y_1'] - row['y_0'])**2 +\n             (row['z_1'] - row['z_0'])**2 ) ** 0.5\n\ntrain['dist'] = train.apply(lambda x: dist(x), axis=1)\ntest['dist'] = test.apply(lambda x: dist(x), axis=1)\n#time: 16min","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Now calculating length of a vector\n#train['length'] = train ['dist'] **2\n#test['length'] = test ['dist'] **2\n# XX, YY, 'ZZ'\ntrain['XX'] = train ['x_0'] * train ['x_1']\ntrain['YY'] = train ['y_0'] * train ['y_1']\ntrain['ZZ'] = train ['z_0'] * train ['z_1']\ntest['XX'] = test ['x_0'] * test ['x_1']\ntest['YY'] = test ['y_0'] * test ['y_1']\ntest['ZZ'] = test ['z_0'] * test ['z_1']\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# XY, XZ, \ntrain['XY'] = train ['x_0'] * train ['y_1']\ntrain['XZ'] = train ['x_0'] * train ['z_1']\ntest['XY'] = test ['x_0'] * test ['y_1']\ntest['XZ'] = test ['x_0'] * test ['z_1']\n\n# YX, 'YZ'\ntrain['YX'] = train ['y_0'] * train ['x_1']\ntrain['YZ'] = train ['y_0'] * train ['z_1']\ntest['YX'] = test ['y_0'] * test ['x_1']\ntest['YZ'] = test ['y_0'] * test ['z_1']\n\n# ZX, ZY\ntrain['ZX'] = train ['z_0'] * train ['x_1']\ntrain['ZY'] = train ['z_0'] * train ['y_1']\ntest['ZX'] = test ['z_0'] * test ['x_1']\ntest['ZY'] = test ['z_0'] * test ['y_1']"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\n# Label Encoding\nfor f in ['molecule_name','type', 'atom_0', 'atom_1']:\n    lbl = LabelEncoder()\n    lbl.fit(list(train[f].values) + list(train[f].values))\n    train[f] = lbl.transform(list(train[f].values))\n   # test[f] = lbl.transform(list(test[f].values))"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X=train.drop(['scalar_coupling_constant'], axis=1)\n#X.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y=train.scalar_coupling_constant\n#y.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into validation and training data\n#train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\n# Specify Model\niowa_model = GradientBoostingRegressor(random_state=0,max_depth=5,n_estimators=500,\n                                       learning_rate=0.1 ,min_samples_leaf=1,max_leaf_nodes=None)\n# 2.(random_state=0,max_depth=5,n_estimators=500,learning_rate=0.1 ,min_samples_leaf=1,max_leaf_nodes=5)\n# 3.(random_state=0,max_depth=3,n_estimators=100,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n# 3.(random_state=0,max_depth=3,n_estimators=110,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n# 3.(random_state=0,max_depth=3,n_estimators=115,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n# 3.(random_state=0,max_depth=3,n_estimators=120,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n# 3.(random_state=0,max_depth=3,n_estimators=125,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n\n# Fit Model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\n#print ('val_y', val_y)\n#print ('val_predictions', val_predictions)\nval_mae= mean_absolute_error(val_y, val_predictions)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format((val_mae)))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\n# Specify Model\niowa_model = GradientBoostingRegressor(random_state=0,max_depth=3,n_estimators=130,\n                                       learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n# 3.(random_state=0,max_depth=3,n_estimators=130,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n# Fit Model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\n#print ('val_y', val_y)\n#print ('val_predictions', val_predictions)\nval_mae= mean_absolute_error(val_y, val_predictions)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format((val_mae)))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\n# Specify Model\niowa_model = GradientBoostingRegressor(random_state=0,max_depth=3,n_estimators=135,\n                                       learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n# 3.(random_state=0,max_depth=3,n_estimators=135,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n\n# Fit Model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\n#print ('val_y', val_y)\n#print ('val_predictions', val_predictions)\nval_mae= mean_absolute_error(val_y, val_predictions)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format((val_mae)))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\n# Specify Model\niowa_model = GradientBoostingRegressor(random_state=0,max_depth=3,n_estimators=140,\n                                       learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n# 3.(random_state=0,max_depth=3,n_estimators=140,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n# Fit Model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\n#print ('val_y', val_y)\n#print ('val_predictions', val_predictions)\nval_mae= mean_absolute_error(val_y, val_predictions)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format((val_mae)))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\n# Specify Model\niowa_model = GradientBoostingRegressor(random_state=0,max_depth=3,n_estimators=145,\n                                       learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n# 3.(random_state=0,max_depth=3,n_estimators=145,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n\n# Fit Model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\n#print ('val_y', val_y)\n#print ('val_predictions', val_predictions)\nval_mae= mean_absolute_error(val_y, val_predictions)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format((val_mae)))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\n# Specify Model\niowa_model = GradientBoostingRegressor(random_state=0,max_depth=3,n_estimators=150,\n                                       learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n# 3.(random_state=0,max_depth=5,n_estimators=150,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n\n\n# Fit Model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\n#print ('val_y', val_y)\n#print ('val_predictions', val_predictions)\nval_mae= mean_absolute_error(val_y, val_predictions)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format((val_mae)))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\n# Specify Model\niowa_model = GradientBoostingRegressor(random_state=0,max_depth=3,n_estimators=200,\n                                       learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n# 3.(random_state=0,max_depth=5,n_estimators=200,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n\n\n# Fit Model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\n#print ('val_y', val_y)\n#print ('val_predictions', val_predictions)\nval_mae= mean_absolute_error(val_y, val_predictions)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format((val_mae)))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\n# Specify Model\niowa_model = GradientBoostingRegressor(random_state=0,max_depth=3,n_estimators=300,\n                                       learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n# 3.(random_state=0,max_depth=5,n_estimators=200,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n\n\n# Fit Model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\n#print ('val_y', val_y)\n#print ('val_predictions', val_predictions)\nval_mae= mean_absolute_error(val_y, val_predictions)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format((val_mae)))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%%time\n# Specify Model\niowa_model = GradientBoostingRegressor(random_state=0,max_depth=3,n_estimators=300,\n                                       learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n# 3.(random_state=0,max_depth=5,n_estimators=200,learning_rate=0.08 ,min_samples_leaf=1,max_leaf_nodes=5)\n\n\n\n# Fit Model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\n#print ('val_y', val_y)\n#print ('val_predictions', val_predictions)\nval_mae= mean_absolute_error(val_y, val_predictions)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format((val_mae)))"},{"metadata":{},"cell_type":"markdown","source":" # max_leaf_Nodes"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 1).\n\n# To improve accuracy, create a new Random Forest model which you will train on all training data\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y,preds_val)\n    return(mae)\n\ncandidate_max_leaf_nodes = [5, 25, 50,75,100] #[2,2,2,2,2]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor max_leaf_nodes in candidate_max_leaf_nodes:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 2).\n%%time\n\n# To improve accuracy, create a new Random Forest model which you will train on all training data\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y,preds_val)\n    return(mae)\n\ncandidate_max_leaf_nodes = [125,150,175,200,225]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor max_leaf_nodes in candidate_max_leaf_nodes:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 3).\n%%time\n# To improve accuracy, create a new Random Forest model which you will train on all training data\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y,preds_val)\n    return(mae)\n\ncandidate_max_leaf_nodes = [250,275,300,325,350]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor max_leaf_nodes in candidate_max_leaf_nodes:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 4).\n%%time\n# To improve accuracy, create a new Random Forest model which you will train on all training data\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y,preds_val)\n    return(mae)\n\ncandidate_max_leaf_nodes = [375,400,425,450,475,500]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor max_leaf_nodes in candidate_max_leaf_nodes:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))"},{"metadata":{},"cell_type":"markdown","source":"# min_samples_leaf"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#1).\n%%time\n# To improve accuracy, create a new Random Forest model which you will train on all training data\ndef get_mae(min_samples_leaf, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(min_samples_leaf=min_samples_leaf, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\ncandidate_min_samples_leaf = [1,2,3,4,5]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor min_samples_leaf in candidate_min_samples_leaf:\n    my_mae = get_mae(min_samples_leaf, train_X, val_X, train_y, val_y)\n    print(\"min_samples_leaf: %d  \\t\\t Mean Absolute Error:  %d\" %(min_samples_leaf, my_mae))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#2).\n%%time\n# To improve accuracy, create a new Random Forest model which you will train on all training data\ndef get_mae(min_samples_leaf, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(min_samples_leaf=min_samples_leaf, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\ncandidate_min_samples_leaf = [6,7,8,9,10]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor min_samples_leaf in candidate_min_samples_leaf:\n    my_mae = get_mae(min_samples_leaf, train_X, val_X, train_y, val_y)\n    print(\"min_samples_leaf: %d  \\t\\t Mean Absolute Error:  %d\" %(min_samples_leaf, my_mae))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#3).\n%%time\n# To improve accuracy, create a new Random Forest model which you will train on all training data\ndef get_mae(min_samples_leaf, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(min_samples_leaf=min_samples_leaf, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\ncandidate_min_samples_leaf = [15,25, 50,75] \n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor min_samples_leaf in candidate_min_samples_leaf:\n    my_mae = get_mae(min_samples_leaf, train_X, val_X, train_y, val_y)\n    print(\"min_samples_leaf: %d  \\t\\t Mean Absolute Error:  %d\" %(min_samples_leaf, my_mae))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#4).\n%%time\n# To improve accuracy, create a new Random Forest model which you will train on all training data\ndef get_mae(min_samples_leaf, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(min_samples_leaf=min_samples_leaf, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\ncandidate_min_samples_leaf = [100,150,200, 300, 400]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor min_samples_leaf in candidate_min_samples_leaf:\n    my_mae = get_mae(min_samples_leaf, train_X, val_X, train_y, val_y)\n    print(\"min_samples_leaf: %d  \\t\\t Mean Absolute Error:  %d\" %(min_samples_leaf, my_mae))"},{"metadata":{},"cell_type":"markdown","source":"# n_estimators"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#2).\n%%time\ndef get_mae(n_estimators, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(n_estimators=n_estimators, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\nfind_n_estimators = [100,200,300,400,500]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor n_estimators in find_n_estimators:\n    my_mae = get_mae(n_estimators, train_X, val_X, train_y, val_y)\n    print(\"n_estimators: %d  \\t\\t Mean Absolute Error:  %d\" %(n_estimators, my_mae))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#3).\n#%%time\ndef get_mae(n_estimators, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(n_estimators=n_estimators, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\nfind_n_estimators = [110,115,120,125] \n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor n_estimators in find_n_estimators:\n    my_mae = get_mae(n_estimators, train_X, val_X, train_y, val_y)\n    print(\"n_estimators: %d  \\t\\t Mean Absolute Error:  %d\" %(n_estimators, my_mae))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#4).\n%%time\ndef get_mae(n_estimators, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(n_estimators=n_estimators, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\nfind_n_estimators = [130,135,140,145,150]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor n_estimators in find_n_estimators:\n    my_mae = get_mae(n_estimators, train_X, val_X, train_y, val_y)\n    print(\"n_estimators: %d  \\t\\t Mean Absolute Error:  %d\" %(n_estimators, my_mae))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#5).\n%%time\ndef get_mae(n_estimators, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(n_estimators=n_estimators, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\nfind_n_estimators = [155, 160,165,170,175]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor n_estimators in find_n_estimators:\n    my_mae = get_mae(n_estimators, train_X, val_X, train_y, val_y)\n    print(\"n_estimators: %d  \\t\\t Mean Absolute Error:  %d\" %(n_estimators, my_mae))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#5).\n%%time\ndef get_mae(n_estimators, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(n_estimators=n_estimators, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\nfind_n_estimators = [180, 185,190,195,200]\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor n_estimators in find_n_estimators:\n    my_mae = get_mae(n_estimators, train_X, val_X, train_y, val_y)\n    print(\"n_estimators: %d  \\t\\t Mean Absolute Error:  %d\" %(n_estimators, my_mae))"},{"metadata":{},"cell_type":"markdown","source":"# learning_rate"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def get_mae(learning_rate, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(learning_rate=learning_rate, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val =( model.predict(val_X))\n    mae = (mean_absolute_error(val_y, preds_val))\n    return(mae)\n\nfind_learning_rate = [0.08,0.09,0.1,0.2] #[2,2,2,2]\n# 1. 9=0.01,2. 4=0.02,3. 3=0.03, 4. 2=0.04, 5. 2=0.05, 6. 2=0.06, 7. 2=0.07\n    \n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor learning_rate in find_learning_rate:\n    my_mae = get_mae(float(learning_rate), train_X, val_X, train_y, val_y)\n    print(\"learning_rate: %d  \\t\\t Mean Absolute Error:  %d\" %(learning_rate, my_mae))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def get_mae(learning_rate, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(learning_rate=learning_rate, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\nfind_learning_rate =[0.3,0.4,0.5,0.6] # [2,2,2,2]\n# 1. 9=0.01,2. 4=0.02,3. 3=0.03, 4. 2=0.04, 5. 2=0.05, 6. 2=0.06, 7. 2=0.07\n    \n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor learning_rate in find_learning_rate:\n    my_mae = get_mae(learning_rate, train_X, val_X, train_y, val_y)\n    print(\"learning_rate: %d  \\t\\t Mean Absolute Error:  %d\" %(learning_rate, my_mae))"},{"metadata":{},"cell_type":"markdown","source":"# max_depth"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def get_mae(max_depth, train_X, val_X, train_y, val_y):\n    model = GradientBoostingRegressor(max_depth=max_depth, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\nfind_max_depth =[3,4,5] #[2,2,2]\n# 1. 9=0.01,2. 4=0.02,3. 3=0.03, 4. 2=0.04, 5. 2=0.05, 6. 2=0.06, 7. 2=0.07\n    \n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor max_depth in find_max_depth:\n    my_mae = get_mae(max_depth, train_X, val_X, train_y, val_y)\n    print(\"max_depth: %d  \\t\\t Mean Absolute Error:  %d\" %(max_depth, my_mae))"},{"metadata":{"trusted":true},"cell_type":"code","source":"molecules = train.pop('molecule_name')\ntest = test.drop('molecule_name', axis=1)\n\ny = train.pop('scalar_coupling_constant')\n\n# Label Encoding\nfor f in ['type', 'atom_0', 'atom_1']:\n    lbl = LabelEncoder()\n    lbl.fit(list(train[f].values) + list(train[f].values))\n    train[f] = lbl.transform(list(train[f].values))\n    test[f] = lbl.transform(list(test[f].values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check the run time, below cell"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nyoof = np.zeros(len(train))\nyhat = np.zeros(len(test))\n\nn_splits = 3\ngkf = GroupKFold(n_splits=n_splits) # we're going to split folds by molecules\n\nfold = 0\nfor train_index, test_index in gkf.split(train, y, groups=molecules):\n    fold += 1\n    print(f'fold {fold} of {n_splits}')\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = train.values[train_index], train.values[test_index]\n    y_train, y_test = y.values[train_index], y.values[test_index]\n    #print(X_train, X_test, y_train, y_test)\n    reg = GradientBoostingRegressor(n_estimators=600, learning_rate=0.1 ,\n                                max_depth=5, \n                                min_samples_leaf=1,\n                                random_state=0)\n                                #n_jobs=-1) # RandomForestRegressor , LGBMClassifier, 250,9,3,4\n    reg.fit(X_train, y_train)\n    yoof[test_index] = reg.predict(X_test)\n    yhat += reg.predict(test)\n\nyhat /= n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Try different numbers of n_estimators - this will take a minute or so\nestimators = np.arange(10, 200, 10)\nscores = []\nfor n in estimators:\n    reg.set_params(n_estimators=n)\n    reg.fit(X_in, y_in)\n    scores.append(reg.score(X_oof, y_oof))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.metrics import mean_absolute_error\nscore = mean_absolute_error(yoof, yhat)\nprint(f'Score: {score:0.3f}')"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='id')#, nrows=20_000)\n\nbenchmark = sample_submission.copy()\nbenchmark['scalar_coupling_constant'] = yhat\nbenchmark.to_csv('atomic_distance_benchmark.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data = pd.DataFrame(y)\nplot_data.index.name = 'id'\nplot_data['yhat'] = yoof\nplot_data['type'] = pd.read_csv('../input/train.csv', index_col='id', usecols=['id', 'type'])\n\ndef plot_oof_preds(ctype, llim, ulim):\n        plt.figure(figsize=(6,6))\n        sns.scatterplot(x='scalar_coupling_constant',y='yhat',\n                        data=plot_data.loc[plot_data['type']==ctype,\n                        ['scalar_coupling_constant', 'yhat']]);\n        plt.xlim((llim, ulim))\n        plt.ylim((llim, ulim))\n        plt.plot([llim, ulim], [llim, ulim])\n        plt.xlabel('scalar_coupling_constant')\n        plt.ylabel('predicted')\n        plt.title(f'{ctype}', fontsize=18)\n        plt.show()\n\nplot_oof_preds('1JHC', 0, 250)\nplot_oof_preds('1JHN', 0, 100)\nplot_oof_preds('2JHC', -50, 50)\nplot_oof_preds('2JHH', -50, 50)\nplot_oof_preds('2JHN', -25, 25)\nplot_oof_preds('3JHC', -25, 100)\nplot_oof_preds('3JHH', -20, 20)\nplot_oof_preds('3JHN', -15, 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}