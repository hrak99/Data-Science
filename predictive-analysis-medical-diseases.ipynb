{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n\n# Maximazing the Dsiplay\npd.set_option('display.max_columns', None) \npd.set_option('display.max_rows', None)\npd.set_option('display.width', None)\n\n\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PREPROCESSING"},{"metadata":{},"cell_type":"markdown","source":"### The preprocessing basically involves a data analysis, so that data should be ready for algorithm designing. It involves data cleaning,convertion from object to numeric, selecting features, splitting  the data for training and validation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the Columns\ncol_names=['age','gender','Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_excel('/kaggle/input/Medical_diseases_data.xlsx',names=col_names)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Exploring the type of each column for numeric conversaion if required\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting object to numeric value for desgining algorithms\nfrom sklearn.preprocessing import LabelEncoder\nfor f in ['gender']:\n    lbl = LabelEncoder()\n    lbl.fit(list(df[f].values) + list(df[f].values))\n    df[f] = lbl.transform(list(df[f].values))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\nX[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\ny [0:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MULTI-LABEL CLASSIFICAITON"},{"metadata":{},"cell_type":"markdown","source":"### In this classification,we have a set of target variables. Thats why it becomes multi label. There are many ways to solve this classification.\n\n### Basically, there are three methods to solve a multi-label classification problem,Problem Transformation, Adapted Algorithm and Ensemble approaches"},{"metadata":{},"cell_type":"markdown","source":"## Base Classifiers\n\n## SVM\n\n### A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane.\n\n## Random Forest\n\n### A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting, \"Ensemble Technique\"\n\n## 6. C4.5 (DECISION TREE CLASSIFIER)\n\n### C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.\n\n### scikit-learn uses an optimised version of the CART algorithm. Basically, CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node. So we will use CART based Decision Tree Classifier\n\n### For multitask, we have to use all the base classifiers one by one with transform method, adapted algortihms and ensemble approaches"},{"metadata":{},"cell_type":"markdown","source":"## CROSS VALIDATION (STRATIFICATION)"},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation is used to assess the predictive performance of the models and and to judge how they perform outside the sample to a new data set also known as test data. \n\n### The splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value. This is called stratified cross-validation.\n\n### The purpose of cross validation as described below is to extract data in training and testing form with equal folds.\n\n### For multitask, we have to use Multi-label stratified cross validation. There is another method known as iterative, very similar to train-test split. We will divide the data into training and testing sets for both methods, so that we can see the difference in results.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Problem Transformation"},{"metadata":{},"cell_type":"markdown","source":"### In this method, we will try to transform our multi-label problem into single-label problem(s). This method can be carried out in three different ways. \n\n### 1. Binary Relevance\n### 2. Classifier Chains\n### 3. Label Powerset\n\n### We will apply Classifier chains and Label Powerset through base classifiers like SVM, Random Forest & C4.5(Decision Tree)"},{"metadata":{},"cell_type":"markdown","source":"## 1. CLASSIFIER CHAINS\n\n### This class provides implementation of Jesse Read’s problem transformation method called Classifier Chains. For L labels it trains L classifiers ordered in a chain according to the Bayesian chain rule.\n\n### The first classifier is trained just on the input space, and then each next classifier is trained on the input space and all previous classifiers in the chain."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install iterative-stratification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install iterative-stratification\nprint ('CLASSIFIER CHAINS')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training an test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\n# Designing \n\n# using classifier chains\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.naive_bayes import GaussianNB\n\n# initialize classifier chains multi-label classifier\n# with a gaussian naive bayes base classifier\nclassifier = ClassifierChain(GaussianNB())\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\nprint (\"PREDICTIVE MODELLING OUTCOME\")\nprint (predictions)\nprint ('=====================================')\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions),'The above is classification_report for Stratification method')\nfrom sklearn.metrics import f1_score,recall_score, precision_score\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('=====================================')\nprint ('Accuracy (Stratification):', accuracy_score(y_test,predictions))\nprint ('Hamming Loss (Stratification):' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')\n\n\n# ITERATIVE METHOD\nprint ('ITERATIVE METHOD FOR MULTI-CLASS SPLITTING')\nprint ('=====================================')\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training an test set\nfrom skmultilearn.model_selection import iterative_train_test_split\nX_train, y_train, X_test, y_test = iterative_train_test_split(X, y,test_size = 0.2)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n# Designing \n\n# using classifier chains\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.naive_bayes import GaussianNB\n\n# initialize classifier chains multi-label classifier\n# with a gaussian naive bayes base classifier\nclassifier = ClassifierChain(GaussianNB())\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\nprint (\"PREDICTIVE MODELLING OUTCOME\")\nprint (predictions)\nprint ('=====================================')\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions),'The above is classification report for iterative')\nprint ('=====================================')\nfrom sklearn.metrics import f1_score,recall_score, precision_score\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('=====================================')\nprint ('Accuracy (Iterative):', accuracy_score(y_test,predictions))\nprint ('Hamming Loss (Iterative):' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\n### Both methods are giving same results on test data. Stratfication is slightly better. So we will opt this technique for rest of the classifiers.\n\n### Now we will apply base classifiers on 'Classifier Chain'"},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('SVM (Classifier Chains)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.svm import SVC\n\n# initialize Classifier Chain multi-label classifier\n# with an SVM classifier\n# SVM in scikit only supports the X matrix in sparse representation\n\n\n\nclassifier = ClassifierChain(\n    classifier = SVC(),\n    require_dense = [False, True]\n)\n\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, f1_score , precision_score,recall_score  \nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')\n\nprint ('RANDOM FOREST (Classifier Chains)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\nfrom skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.ensemble import RandomForestClassifier\n\n# initialize ClassifierChain multi-label classifier with a RandomForest\nclassifier = ClassifierChain(\n    classifier = RandomForestClassifier(n_estimators=100),\n    require_dense = [False, True]\n)\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')\nprint ('DECISION TREE CLASSIFIER, C4.5 (Classifier Chains)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train, y_train)\n\n# predict\npredictions = clf.predict(X_test)\n\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. LABEL POWERSET\n\n### Label Powerset is a problem transformation approach to multi-label classification that transforms a multi-label problem to a multi-class problem with 1 multi-class classifier trained on all unique label combinations found in the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# using Label Powerset\nfrom skmultilearn.problem_transform import LabelPowerset\nprint ('SVM ( LabelPowerset)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\nfrom skmultilearn.problem_transform import LabelPowerset\nfrom sklearn.svm import SVC\n\n# initialize Classifier Chain multi-label classifier\n# with an SVM classifier\n# SVM in scikit only supports the X matrix in sparse representation\n\n\n\nclassifier = LabelPowerset(\n    classifier = SVC(),\n    require_dense = [False, True]\n)\n\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')\n\nprint ('RANDOM FOREST ( LabelPowerset)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\nfrom skmultilearn.problem_transform import LabelPowerset\nfrom sklearn.ensemble import RandomForestClassifier\n\n# initialize ClassifierChain multi-label classifier with a RandomForest\nclassifier = LabelPowerset(\n    classifier = RandomForestClassifier(n_estimators=100),\n    require_dense = [False, True]\n)\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')\nprint ('DECISION TREE CLASSIFIER, C4.5 ( LabelPowerset)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\nfrom sklearn import tree\nclassifier = LabelPowerset(\n    classifier = tree.DecisionTreeClassifier(),\n    require_dense = [False, True]\n)\n\n\n# train\nclassifier = classifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. The Random k-labeLsets method (RAkEL)\n### It may use less classifiers than Binary Relevance and still generalize label relations while not underfitting like LabelPowerset. Using random approach is not very probable to draw an optimal label space division"},{"metadata":{"trusted":true},"cell_type":"code","source":"from skmultilearn.ensemble import RakelD\n\nprint ('SVM ( RakelD)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\nfrom sklearn.svm import SVC\n\n# initialize Classifier Chain multi-label classifier\n# with an SVM classifier\n# SVM in scikit only supports the X matrix in sparse representation\n\n\n\n\nclassifier = RakelD(\n    base_classifier=SVC(),\n    base_classifier_require_dense=[False, True],\n    labelset_size=4\n)\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')\n\nprint ('RANDOM FOREST (RakelD)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# initialize ClassifierChain multi-label classifier with a RandomForest\nclassifier = RakelD(\n    base_classifier=RandomForestClassifier(n_estimators=100),\n    base_classifier_require_dense=[False, True],\n    labelset_size=4\n)\n\n\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')\nprint ('DECISION TREE CLASSIFIER, C4.5(RakelD)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\nfrom sklearn import tree\nclassifier = RakelD(\n    base_classifier=tree.DecisionTreeClassifier(),\n    base_classifier_require_dense=[False, True],\n    labelset_size=4\n)\n\n\n# train\nclassifier = classifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.Multi-label Naïve Bayesian (ML-NB)\n### The multinomial Naive Bayes classifier is suitable for classification with discrete features. It is also  a base classifier, so it can used with one of the transformation methods like classifier chain or Label Powetset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclassifier = ClassifierChain(\n    classifier = MultinomialNB(),\n    require_dense = [False, True]\n)\n\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 5.MLkNN (ADAPTED ALGORITHM)\n\n### MLkNN builds uses k-NearestNeighbors find nearest examples to a test class and uses Bayesian inference to select assigned labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('MLkNN')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training an test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n# MLkNN \n\nfrom skmultilearn.adapt import MLkNN\n\nclassifier = MLkNN(k=10,s=1)\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\nprint ('=====================================')\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. BINARY RELEVANCE kNN (Adapted)\n\n### Binary Relevance multi-label classifier based on k-Nearest Neighbors method. This version of the classifier assigns the labels that are assigned to at least half of the neighbor"},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('BINARY RELEVANCE kNN')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\nfrom skmultilearn.adapt import BRkNNaClassifier\n\nclassifier = BRkNNaClassifier(k=10)\n\n# train\nclassifier.fit(X_train, y_train)\n\n# predict\npredictions = classifier.predict(X_test)\n\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Ensemble of Classifier Chains (ECC)\n\n### In Ensemble of Classifier Chains (ECC) several CC classifiers can be trained with random order of chains (i.e. random order of labels) on a random subset of data set. Labels of a new instance are predicted by each classifier separately. After that, the total number of predictions or \"votes\" is counted for each label.\n\n### There are two ways to done it. LabelSpacePartitioningClassifier & Majority Voting Classifier.\n\n### The label space is partitioned into separate sub label spaces, for example by constructing a label graph and applying a graph clustering/community detection algorithm. A base multi-label subclassifier is trained on each subspace. The result is predicted with each of the subclassifies and we take the sum This method adapts the classifiers according to the label space and requires less classifiers than binary relevance. On the downside, the label combination has to be present in the training dataset in order to be predicted and partitioning might prevent correct classification of certain label combinations.\n\n### Majority Voting Classifier is similar to the Label Space Partitioning Classifier but the majority vote is used instead. We will use both to examine the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"from skmultilearn.cluster import LabelCooccurrenceGraphBuilder\ngraph_builder = LabelCooccurrenceGraphBuilder(weighted=True,\n                                              include_self_edges=False)\n\nlabel_names=[i for i in range(3)]\nedge_map = graph_builder.transform(y_train)\nprint(\"{} labels, {} edges\".format(len(label_names), len(edge_map)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skmultilearn.cluster import NetworkXLabelGraphClusterer\n\n# we define a helper function for visualization purposes\ndef to_membership_vector(partition):\n    return {\n        member :  partition_id\n        for partition_id, members in enumerate(partition)\n        for member in members\n    }\nclusterer = NetworkXLabelGraphClusterer(graph_builder, method='louvain')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"partition = clusterer.fit_predict(X_train,y_train)\nmembership_vector = to_membership_vector(partition)\nprint('There are', len(partition),'clusters')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import networkx as nx\nnames_dict = dict(enumerate(x for x in label_names))\nimport matplotlib.pyplot as plt\n%matplotlib inline\nnx.draw(\n    clusterer.graph_,\n    pos=nx.spring_layout(clusterer.graph_,k=4),\n    labels=names_dict,\n    with_labels = True,\n    width = [10*x/y_train.shape[0] for x in clusterer.weights_['weight']],\n    node_color = [membership_vector[i] for i in range(y_train.shape[1])],\n    cmap=plt.cm.viridis,\n    node_size=250,\n    font_size=10,\n    font_color='white',\n    alpha=0.8\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LabelSpacePartitioningClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from skmultilearn.ensemble import LabelSpacePartitioningClassifier\nfrom skmultilearn.cluster import LabelCooccurrenceGraphBuilder\nfrom skmultilearn.cluster import NetworkXLabelGraphClusterer\n\nprint ('SVM ( ECC)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\nfrom sklearn.svm import SVC\n\n# initialize Classifier Chain multi-label classifier\n# with an SVM classifier\n# SVM in scikit only supports the X matrix in sparse representation\n\n# Label Graph\n# When the label space is large, we can try to explore it using graph methods.\n# Each label is a node in the graph and an edge exists when labels co-occur, \n# weighted by the frequency of co-occurrence.\n\ngraph_builder = LabelCooccurrenceGraphBuilder(weighted=True,\n                                              include_self_edges=False)\n\nedge_map = graph_builder.transform(y_train)\n\nclassifier = LabelSpacePartitioningClassifier(\n    classifier = ClassifierChain(\n    classifier = SVC(),\n    require_dense = [False, True]\n),\n    clusterer  = NetworkXLabelGraphClusterer(graph_builder, method='louvain')\n)\n\n# train\nclassifier.fit(X_train,y_train)\n\n# predict\npredictions=classifier.predict(X_test)\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')\n\nprint ('RANDOM FOREST (ECC)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\n\ngraph_builder = LabelCooccurrenceGraphBuilder(weighted=True,\n                                              include_self_edges=False)\n\nedge_map = graph_builder.transform(y_train)\n\nclassifier = LabelSpacePartitioningClassifier(\n    classifier = ClassifierChain(\n    classifier = RandomForestClassifier(n_estimators=100),\n    require_dense = [False, True]\n),\n    clusterer  = NetworkXLabelGraphClusterer(graph_builder, method='louvain')\n)\n\n# train\nclassifier.fit(X_train,y_train)\n\n# predict\npredictions=classifier.predict(X_test)\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')\nprint ('DECISION TREE CLASSIFIER, C4.5(ECC)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\ngraph_builder = LabelCooccurrenceGraphBuilder(weighted=True,\n                                              include_self_edges=False)\n\nedge_map = graph_builder.transform(y_train)\n\nclassifier = LabelSpacePartitioningClassifier(\n    classifier = ClassifierChain(\n    classifier = tree.DecisionTreeClassifier(),\n    require_dense = [False, True]\n),\n    clusterer  = NetworkXLabelGraphClusterer(graph_builder, method='louvain')\n)\n\n# train\nclassifier.fit(X_train,y_train)\n\n# predict\npredictions=classifier.predict(X_test)\n\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MajorityVotingClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from skmultilearn.ensemble import MajorityVotingClassifier\nfrom skmultilearn.cluster import LabelCooccurrenceGraphBuilder\nfrom skmultilearn.cluster import NetworkXLabelGraphClusterer\n\nprint ('SVM ( ECC)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\nfrom sklearn.svm import SVC\n\n# initialize Classifier Chain multi-label classifier\n# with an SVM classifier\n# SVM in scikit only supports the X matrix in sparse representation\n\n# Label Graph\n# When the label space is large, we can try to explore it using graph methods.\n# Each label is a node in the graph and an edge exists when labels co-occur, \n# weighted by the frequency of co-occurrence.\n\ngraph_builder = LabelCooccurrenceGraphBuilder(weighted=True,\n                                              include_self_edges=False)\n\nedge_map = graph_builder.transform(y_train)\n\nclassifier = MajorityVotingClassifier(\n    classifier = ClassifierChain(\n    classifier = SVC(),\n    require_dense = [False, True]\n),\n    clusterer  = NetworkXLabelGraphClusterer(graph_builder, method='louvain')\n)\n\n# train\nclassifier.fit(X_train,y_train)\n\n# predict\npredictions=classifier.predict(X_test)\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')\n\nprint ('RANDOM FOREST (ECC)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\n\ngraph_builder = LabelCooccurrenceGraphBuilder(weighted=True,\n                                              include_self_edges=False)\n\nedge_map = graph_builder.transform(y_train)\n\nclassifier = MajorityVotingClassifier(\n    classifier = ClassifierChain(\n    classifier = RandomForestClassifier(n_estimators=100),\n    require_dense = [False, True]\n),\n    clusterer  = NetworkXLabelGraphClusterer(graph_builder, method='louvain')\n)\n\n# train\nclassifier.fit(X_train,y_train)\n\n# predict\npredictions=classifier.predict(X_test)\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')\nprint ('DECISION TREE CLASSIFIER, C4.5(ECC)')\nprint ('=====================================')\n# STRATIFICATION METHOD\nprint (\"STRATIFICATION CROSS VALIDATION\")\nprint ('=====================================')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n\n# Selecting Features \nfeature_df = df[['age','gender',\\\n           'Systolic (mmHg)','Diastolic (mmHg)','Mean Arterial Pressure (MAP) (mmHg)',\\\n           'Fasting Blood Glucose (mg/dL)','Random (mg/dL)', 'HbA1C (%)',\\\n           'eAG (Estimate Average Glucose) Level (mg/dL)','Total cholesterol (mg/dL)',' HDL (mg/dL)',\\\n           'LDL (mg/dL)','Triglyceride  (mg/dL)','VLDL  (mg/dL)','Serum Creatinine  (mg/dL)','BUN  (mg/dL)']]\nX = np.asarray(feature_df)\n\n# target variable\ny = np.asarray(df[['Diabetes_Mllitus','Hypertension Diagnosis','Hyperlipidemia Diagnosis']])\n\n# Splitting data into training and test set\nfor train_index, test_index in mskf.split(X, y):\n   #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nprint ('=====================================')\n\n\ngraph_builder = LabelCooccurrenceGraphBuilder(weighted=True,\n                                              include_self_edges=False)\n\nedge_map = graph_builder.transform(y_train)\n\nclassifier = MajorityVotingClassifier(\n    classifier = ClassifierChain(\n    classifier = tree.DecisionTreeClassifier(),\n    require_dense = [False, True]\n),\n    clusterer  = NetworkXLabelGraphClusterer(graph_builder, method='louvain')\n)\n\n# train\nclassifier.fit(X_train,y_train)\n\n# predict\npredictions=classifier.predict(X_test)\n\n\n# Evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\nprint ('EVALUATION')\nprint (classification_report(y_test,predictions))\nprint ('=====================================')\nprint ('f1_score (micro)',f1_score(y_test, predictions, average='micro') )\nprint ('recall_score (micro)',recall_score(y_test, predictions, average='micro') )\nprint ('precision_score (micro)',precision_score(y_test, predictions, average='micro') )\nprint ('f1_score (macro)',f1_score(y_test, predictions, average='macro') )\nprint ('recall_score (macro)',recall_score(y_test, predictions, average='macro') )\nprint ('precision_score (macro)',precision_score(y_test, predictions, average='macro') )\nprint ('Accuracy:', accuracy_score(y_test,predictions))\nprint ('Hamming Loss:' , metrics.hamming_loss(y_test, predictions))\nprint ('=====================================')\n\nprint ('Note: In Multitask, the accuracy is always be in the form of sub-accuracy.')\nprint ('We used the same way as we do for finding the accuracy through sklearn metric.')\nprint ('=====================================')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From above results. we can't say that one ensemble method is better than the other. It depends on base classifiers. For example, SVM is giving same results on both methods. Random Forest is giving better results on LabelSpacePartitioningClassifier while Decision Tree is good for MajorVotingClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}